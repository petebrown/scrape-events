{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import re\n",
    "\n",
    "def get_game_list():\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/petebrown/update-player-stats/main/data/players_df.csv\")\n",
    "    df.sb_game_id = df.sb_game_id.str.replace(\"tpg\", \"\")\n",
    "    df[\"url\"] = df.apply(lambda x: f\"https://www.soccerbase.com/matches/additional_information.sd?id_game={x.sb_game_id}\", axis=1)\n",
    "\n",
    "    games = df[[\"url\", \"venue\"]].drop_duplicates().to_dict(\"records\")\n",
    "\n",
    "    return games\n",
    "\n",
    "def create_record(game_id, player_id, min_off, min_so, min_on):\n",
    "    record = {\n",
    "        \"game_id\": game_id,\n",
    "        \"player_id\": player_id,\n",
    "        \"min_on\": min_on,\n",
    "        \"min_off\": min_off,\n",
    "        \"min_so\": min_so\n",
    "        }\n",
    "    return record\n",
    "\n",
    "def get_player_id(event_text):\n",
    "    player_id = event_text.find(\"a\")[\"href\"].split(\"=\")[1]\n",
    "    return player_id\n",
    "\n",
    "def scrape_starter_events(event):\n",
    "    regex = r\"\\((\\d+)\\)|\\(s\\/o (\\d+)\\)\"\n",
    "    matches = re.search(regex, event)\n",
    "    min_off = matches.group(1)\n",
    "    min_so = matches.group(2)\n",
    "    return min_off, min_so\n",
    "\n",
    "def scrape_sub_events(event):\n",
    "    regex = r\"\\((\\d+)-?(\\d+)?(?:, s\\/o )?(\\d+)?\\)\"\n",
    "    matches = re.search(regex, event)\n",
    "    if matches:\n",
    "        min_on = matches.group(1)\n",
    "        min_off = matches.group(2)\n",
    "        min_so = matches.group(3)\n",
    "        return min_on, min_off, min_so\n",
    "\n",
    "def scrape_events(doc, game_id, side):\n",
    "    match_events = []\n",
    "\n",
    "    subs_off = doc.select(f'.lineup .{side} .firstTeam .replaced')\n",
    "    red_cards = doc.select(f'.lineup .{side} .firstTeam .sendingOff')\n",
    "\n",
    "    starter_events = subs_off + red_cards\n",
    "    sub_events = doc.select(f'.lineup .{side} .reserve tr')\n",
    "\n",
    "    for event in starter_events:\n",
    "        player_id = get_player_id(event)\n",
    "        min_off, min_so = scrape_starter_events(event.text)\n",
    "        \n",
    "        record = create_record(game_id, player_id, min_off, min_so, min_on = None)\n",
    "        \n",
    "        match_events.append(record)\n",
    "\n",
    "    for event in sub_events:\n",
    "        regex = r\"\\((\\d+)-?(\\d+)?(?:, s\\/o )?(\\d+)?\\)\"\n",
    "        matches = re.search(regex, event.text)\n",
    "        if matches:\n",
    "            player_id = get_player_id(event)\n",
    "            min_on, min_off, min_so = scrape_sub_events(event.text)\n",
    "            \n",
    "            record = create_record(game_id, player_id, min_off, min_so, min_on)\n",
    "        \n",
    "            match_events.append(record)\n",
    "\n",
    "    return match_events\n",
    "\n",
    "def get_match_page(game_dict):\n",
    "    url = game_dict[\"url\"]\n",
    "    venue = game_dict[\"venue\"]\n",
    "    game_id = url.split(\"=\")[1]\n",
    "\n",
    "    if venue == \"H\":\n",
    "        side = \"teamA\"\n",
    "    elif venue == \"A\":\n",
    "        side = \"teamB\"\n",
    "    else:\n",
    "        next\n",
    "\n",
    "    r = requests.get(url)\n",
    "    doc = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    events = scrape_events(doc, game_id, side)\n",
    "    return events\n",
    "\n",
    "def async_scraping(scrape_function, urls):\n",
    "    MAX_THREADS = 30\n",
    "\n",
    "    threads = min(MAX_THREADS, len(urls))\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "        results = executor.map(scrape_function, urls)\n",
    "\n",
    "    return results\n",
    "\n",
    "def clean_events_list(input_list):\n",
    "    events = list(input_list)\n",
    "    events = [event for sublist in events for event in sublist]\n",
    "    return events\n",
    "\n",
    "def get_events_list(games):\n",
    "    events = async_scraping(get_match_page, games)\n",
    "    events = clean_events_list(events)\n",
    "    return events\n",
    "\n",
    "def main():\n",
    "    games = get_game_list()\n",
    "\n",
    "    events = get_events_list(games)\n",
    "    \n",
    "    df = pd.DataFrame(events)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/subs-and-reds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape-events-QLBxb4Ds-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f238f90f734aa3d332485710151bec439b74c9bef1e1b084370ddad102d947f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
