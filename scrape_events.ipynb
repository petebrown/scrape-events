{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "MAX_THREADS = 30\n",
    "\n",
    "def get_game_list():\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/petebrown/update-player-stats/main/data/players_df.csv\")\n",
    "\n",
    "    df.sb_game_id = df.sb_game_id.str.replace(\"tpg\", \"\")\n",
    "\n",
    "    df[\"url\"] = df.apply(lambda x: f\"https://www.soccerbase.com/matches/additional_information.sd?id_game={x.sb_game_id}\", axis=1)\n",
    "\n",
    "    games = df[[\"url\", \"venue\"]].drop_duplicates().to_dict(\"records\")\n",
    "\n",
    "    return games\n",
    "\n",
    "def make_record(game_id, venue, player_id, event):\n",
    "    record = {\n",
    "        \"game_id\": game_id,\n",
    "        \"venue\": venue,\n",
    "        \"player_id\": player_id,\n",
    "        \"event_details\": event.text.strip(),\n",
    "        }\n",
    "    return record\n",
    "\n",
    "def get_player_id(sub_text):\n",
    "    player_id = sub_text.find(\"a\")[\"href\"].split(\"=\")[1]\n",
    "    return player_id\n",
    "\n",
    "def get_events(doc, venue, game_id, side):\n",
    "    match_events = []\n",
    "\n",
    "    subs_off = doc.select(f'.lineup .{side} .firstTeam .replaced')\n",
    "    subs_on = doc.select(f'.lineup .{side} .reserve tr:not(.replaced)')\n",
    "    dub_subs = doc.select(f'.lineup .{side} .reserve tr')\n",
    "    red_cards = doc.select(f'.lineup .{side} .sendingOff')\n",
    "\n",
    "    for sub in subs_off:\n",
    "        player_id = get_player_id(sub)\n",
    "        record = make_record(game_id, venue, player_id, sub)\n",
    "        match_events.append(record)\n",
    "\n",
    "    for sub in subs_on:\n",
    "        player_id = get_player_id(sub)\n",
    "        record = make_record(game_id, venue, player_id, sub)\n",
    "        match_events.append(record)\n",
    "\n",
    "    for sub in dub_subs:\n",
    "        if '-' in sub.text:\n",
    "            player_id = get_player_id(sub)\n",
    "\n",
    "            record_1 = make_record(game_id, venue, player_id, sub)\n",
    "            record_2 = make_record(game_id, venue, player_id, sub)\n",
    "\n",
    "            match_events.append(record_1)\n",
    "            match_events.append(record_2)\n",
    "        else:\n",
    "            next\n",
    "\n",
    "    for red in red_cards:\n",
    "        player_id = get_player_id(red)\n",
    "\n",
    "        record = make_record(game_id, venue, player_id, red)\n",
    "        \n",
    "        match_events.append(record)\n",
    "    return match_events\n",
    "\n",
    "def scrape_events(game_dict):\n",
    "    url = game_dict[\"url\"]\n",
    "    venue = game_dict[\"venue\"]\n",
    "    game_id = url.split(\"=\")[1]\n",
    "\n",
    "    if venue == \"H\":\n",
    "        side = \"teamA\"\n",
    "    elif venue == \"A\":\n",
    "        side = \"teamB\"\n",
    "    else:\n",
    "        next\n",
    "\n",
    "    r = requests.get(url)\n",
    "    doc = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    events = get_events(doc, venue, game_id, side)\n",
    "    return events\n",
    "\n",
    "def async_scraping(scrape_function, urls):\n",
    "    threads = min(MAX_THREADS, len(urls))\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "        results = executor.map(scrape_function, urls)\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_events_df():\n",
    "    games = get_game_list()\n",
    "\n",
    "    df = async_scraping(scrape_events, games)\n",
    "    df = list(df)\n",
    "    df = [sub for sublist in df for sub in sublist]\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    df = df[(df.event_details.str.contains(\"\\(\\d+-\\d+\\)\", regex=True)) | (df.event_details.str.contains(\"\\(\\d+\", regex=True)) | (df.event_details.str.contains(\"s/o\"))].copy()\n",
    "    df[\"min_on\"] = df.event_details.str.extract(r\"\\((\\d+)\")\n",
    "    df[\"min_off\"] = df.event_details.str.extract(r\"\\-(\\d+)\\)\")\n",
    "    df[\"min_so\"] = df.event_details.str.extract(r\"s/o (\\d+)\\)\")\n",
    "    df = df[[\"game_id\", \"player_id\", \"min_on\", \"min_off\", \"min_so\", \"event_details\"]].drop_duplicates().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = get_events_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/subs-and-reds.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape-events-QLBxb4Ds-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f238f90f734aa3d332485710151bec439b74c9bef1e1b084370ddad102d947f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
